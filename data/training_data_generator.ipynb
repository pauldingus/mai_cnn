{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-valuation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ee, json, re, csv, os, shutil, concurrent.futures, mysql.connector, csv, time, rasterio\n",
    "from sqlalchemy import create_engine\n",
    "from shapely.geometry import shape, GeometryCollection, MultiPolygon, Point\n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import contextily as ctx\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "cnx = mysql.connector.connect(user='root',password='BMkjM8_)-tN8R33u',host='34.72.234.161',database='mai-database')\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-titanium",
   "metadata": {},
   "source": [
    "## Primary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToGeometry(row):\n",
    "    try:\n",
    "        return row['locShape']\n",
    "    except:\n",
    "        return GeometryCollection()\n",
    "\n",
    "def locShpFromQuery(query):\n",
    "    \n",
    "    colsToKeep = ['Location', 'locGroup', 'bucket', 'country', 'admLvl1', 'lat', 'lon' 'marketDays', 'marketLat', 'marketLon', 'mktShape', 'geometry']\n",
    "    engine = create_engine(\"mysql+mysqlconnector://root:BMkjM8_)-tN8R33u@34.72.234.161:3306/mai-database\")\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "\n",
    "    #reconstruct loc shape by buffering the lat and lon by 250m\n",
    "    df['locShape'] = df.apply(lambda x: Point(x['lon'], x['lat']).buffer(0.0025).simplify(0.0001).wkt, axis=1)\n",
    "    df = df[df['locShape'].notnull()]\n",
    "    df['geometry'] = df.apply(convertToGeometry, axis = 1)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    gdf['marketDays'] = gdf['marketDays'].apply(lambda x: str(x))\n",
    "    return gdf[colsToKeep].rename(columns = {'Location': 'mktID'}).set_crs('EPSG:4326')\n",
    "\n",
    "def transform_to_val_output(index_string):\n",
    "    indices = index_string.strip(\"{}\").replace(' ','').replace(\"'\",'').split(\",\")\n",
    "    indices = [int(index) for index in indices]\n",
    "    output_list = [0] * 7\n",
    "    for index in indices:\n",
    "        output_list[index] = 1\n",
    "    return output_list, indices\n",
    "\n",
    "def getCentroid(f):\n",
    "    return f.setGeometry(f.geometry().centroid())\n",
    "\n",
    "def get_largest_polygon_from_features(feature_collection):\n",
    "    # Function to extract individual polygons from a feature, handling both polygons and multipolygons\n",
    "    def extract_polygons(feature):\n",
    "        geometry = feature.geometry()\n",
    "        # Split multipolygons into individual polygons if necessary\n",
    "        if geometry.type().compareTo('Polygon').Not().And(geometry.type().compareTo('MultiPolygon').eq(1)):\n",
    "            return ee.FeatureCollection(geometry.geometries().map(lambda g: ee.Feature(ee.Geometry(g))))\n",
    "        else:\n",
    "            return ee.FeatureCollection([ee.Feature(geometry)])\n",
    "\n",
    "    # Flatten the feature collection to ensure all geometries are individual polygons\n",
    "    all_polygons = feature_collection.map(extract_polygons).flatten()\n",
    "\n",
    "    # Calculate the area of each polygon and store it as a property\n",
    "    polygons_with_area = all_polygons.map(lambda feature: feature.set('area', feature.geometry().area()))\n",
    "\n",
    "    # Sort the polygons by area in descending order\n",
    "    sorted_polygons = polygons_with_area.sort('area', False)\n",
    "\n",
    "    # Get the largest polygon\n",
    "    largest_polygon = sorted_polygons.first()\n",
    "\n",
    "    # Retrieve information about the largest polygon to the client-side\n",
    "    return largest_polygon\n",
    "\n",
    "\n",
    "def export_image_and_data(image, folder, filename, market_days, market, scale, csv_path, country = 'NA'):\n",
    "    \n",
    "    # export the image\n",
    "    image_file_path = f\"{folder}/{filename}.tif\"\n",
    "    geemap.download_ee_image(image, filename=image_file_path, scale=scale)\n",
    "    time.sleep(0.25)\n",
    "    min_value, max_value, std = get_min_max_std(image_file_path)\n",
    "    if std < 0.001:\n",
    "        print(f\"Failed export: {image_file_path}\")\n",
    "        os.remove(image_file_path)\n",
    "        return\n",
    "    \n",
    "    # add metadata to the csv\n",
    "    new_row = [image_file_path, market_days, market, min_value, max_value, std, country]\n",
    "    with open(csv_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(new_row)\n",
    "        \n",
    "def check_filename_in_csv(filename_to_check, csv_path):\n",
    "    \"\"\"\n",
    "    Checks if a specific filename exists in a specified column of a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        filename_to_check (str): The filename to search for in the CSV.\n",
    "        column_index (int): The index of the column where the filename is expected to be.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the filename is found, False otherwise.\n",
    "    \"\"\"\n",
    "    # Open the CSV file\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # Iterate over each row in the CSV\n",
    "        for row in reader:\n",
    "            # Check if the column index exists in the row\n",
    "            if len(row) > 0:\n",
    "                # Compare the content of the specified column with the filename to check\n",
    "                if filename_to_check in row[0]:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "def get_min_max_std(image_path): # Function to get min and max pixel values from a .tif file\n",
    "    with rasterio.open(image_path) as src:\n",
    "        data = src.read(1)  # Read the first layer\n",
    "        min_val = data.min()\n",
    "        max_val = data.max()\n",
    "        std_dev = np.std(data)\n",
    "    return min_val, max_val, std_dev\n",
    "    \n",
    "def export_loc_data_S2(locRow, pixel_size):\n",
    "    \n",
    "    loc = locRow.iloc[0]['mktID']\n",
    "    country = locRow.iloc[0]['country']\n",
    "    csv_path = './training_data_S2/image_metadata.csv'\n",
    "    tile_size = 128  # 256 pixels by 256 pixels\n",
    "    tile_length = pixel_size * tile_size\n",
    "\n",
    "    original_names = ['weekday_0_mean', 'weekday_1_mean', 'weekday_2_mean',\n",
    "                      'weekday_3_mean', 'weekday_4_mean', 'weekday_5_mean', 'weekday_6_mean']\n",
    "    new_names = ['weekday_0', 'weekday_1', 'weekday_2', \n",
    "                 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6']\n",
    "\n",
    "    locGeo = geemap.geopandas_to_ee(locRow).geometry()\n",
    "    grid = locGeo.coveringGrid(proj=ee.Projection('EPSG:4326').atScale(tile_length))\n",
    "    offsetGrid = locGeo.buffer(tile_size*pixel_size*0.5) \\\n",
    "        .coveringGrid(proj=ee.Projection('EPSG:4326') \\\n",
    "        .translate(tile_length, tile_length) \\\n",
    "        .atScale(tile_length))\n",
    "\n",
    "    overlapGrid = grid.merge(offsetGrid).filterBounds(locGeo)\n",
    "\n",
    "    marketDays = locRow.iloc[0]['marketDays']\n",
    "\n",
    "    #create negative output if result is negative\n",
    "#try:\n",
    "    if marketDays == 'None':\n",
    "        dayOutput = [0,0,0,0,0,0,0]\n",
    "        mktOutput = 0\n",
    "        overlapGrid = overlapGrid.filter(ee.Filter.bounds(fc_all_positives).Not())\n",
    "    else:\n",
    "        dayOutput, marketDayList = transform_to_val_output(marketDays)\n",
    "        mktOutput = 1\n",
    "\n",
    "        #create a feature of the largest market day shape, and limit to only cells containing the centroid\n",
    "        mktShape = json.loads(locRow.iloc[0]['mktShape'])\n",
    "        dayShapes = []\n",
    "        for marketDay in marketDayList:\n",
    "            dayShape = mktShape[f'weekday_{marketDay}']\n",
    "            dayShape = ee.FeatureCollection(dayShape)\n",
    "            dayShape = get_largest_polygon_from_features(dayShape)\n",
    "            dayShapes.append(dayShape)\n",
    "\n",
    "        dayShapes = ee.FeatureCollection(dayShapes)\n",
    "        dayShapes_centroid = dayShapes.geometry().centroid()\n",
    "        overlapGrid = overlapGrid.filterBounds(dayShapes_centroid)\n",
    "\n",
    "    # import diffImg\n",
    "    countryName = locRow.iloc[0]['country']\n",
    "    bucketName = countryName.replace(' ', '').lower()\n",
    "    if countryName in ['Kenya', 'Mozambique']:\n",
    "        diffImg = ee.ImageCollection(f'projects/pauldingus/assets/Candidate_Location/{countryName}/diffImgAll').reduce(ee.Reducer.mean())\n",
    "    else:\n",
    "        diffImg = ee.ImageCollection(f'projects/{bucketName}-candidate-locs/assets/diffImgAll').reduce(ee.Reducer.mean())\n",
    "\n",
    "    # for each remaining cell in the grid, export diffImg\n",
    "    i = 0\n",
    "    for feature in overlapGrid.getInfo()['features']:\n",
    "        filename = f'{loc}_{i}'\n",
    "        i += 1\n",
    "        if check_filename_in_csv(filename, csv_path):\n",
    "            print(f'Image {filename} is already exported.')\n",
    "            continue\n",
    "        else:\n",
    "            cell = ee.Feature(feature).geometry()\n",
    "            image = diffImg.reproject(\"EPSG:3857\").clip(cell).unmask(ee.Image.constant(0)).select(original_names, new_names)\n",
    "            folder = 'training_data_S2/images'\n",
    "            export_image_and_data(image, folder, filename, dayOutput, mktOutput, 10, csv_path, country)\n",
    "#except:\n",
    "    print(f'Failed to export image for {loc}.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-group",
   "metadata": {},
   "source": [
    "## Generating S2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e46496",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * FROM location_file \n",
    "WHERE country NOT IN ('Nigeria')\n",
    "AND to_delete IS NULL \n",
    "AND mktShape IS NOT NULL\n",
    "'''\n",
    "positives = locShpFromQuery(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveCounts = positives.groupby('country').agg({'country':'count'}).rename(columns = {'country': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '''\n",
    "# SELECT * FROM location_file \n",
    "# WHERE country IN ('Mali', 'Chad', 'Sudan', 'Togo', 'Malawi', 'Uganda', 'Niger', 'Kenya', 'Mozambique')\n",
    "# AND to_delete IS NULL \n",
    "# AND mktShape IS NOT NULL\n",
    "# '''\n",
    "# all_positives = locShpFromQuery(query)\n",
    "all_positives = positives\n",
    "\n",
    "query = f'''\n",
    "(SELECT * FROM location_file WHERE country = 'Mali' \n",
    "AND to_delete IS NULL AND mktShape IS NULL  LIMIT {positiveCounts.loc['Mali', 'count']})\n",
    "UNION\n",
    "(SELECT * FROM location_file WHERE country = 'Chad' \n",
    "AND to_delete IS NULL AND mktShape IS NULL  LIMIT {positiveCounts.loc['Chad', 'count']})\n",
    "UNION\n",
    "(SELECT * FROM location_file WHERE country = 'Sudan' \n",
    "AND to_delete IS NULL AND mktShape IS NULL  LIMIT {positiveCounts.loc['Sudan', 'count']})\n",
    "UNION\n",
    "(SELECT * FROM location_file WHERE country = 'Togo' \n",
    "AND to_delete IS NULL AND mktShape IS NULL  LIMIT {positiveCounts.loc['Togo', 'count']})\n",
    "UNION\n",
    "(SELECT * FROM location_file WHERE country = 'Uganda' \n",
    "AND to_delete IS NULL AND mktShape IS NULL  LIMIT {positiveCounts.loc['Uganda', 'count']})\n",
    "UNION\n",
    "(SELECT * FROM location_file WHERE country = 'Niger' \n",
    "AND to_delete IS NULL AND mktShape IS NULL  LIMIT {positiveCounts.loc['Niger', 'count']})\n",
    "UNION\n",
    "(SELECT * FROM location_file WHERE country = 'Kenya' \n",
    "AND to_delete IS NULL AND mktShape IS NULL  LIMIT {positiveCounts.loc['Kenya', 'count']})\n",
    "UNION\n",
    "(SELECT * FROM location_file WHERE country = 'Mozambique' \n",
    "AND to_delete IS NULL AND mktShape IS NULL LIMIT {positiveCounts.loc['Mozambique', 'count']})\n",
    "'''\n",
    "negatives = locShpFromQuery(query)\n",
    "\n",
    "lon_col='marketLon'\n",
    "lat_col='marketLat'\n",
    "fc_all_positives = ee.FeatureCollection(all_positives.apply(lambda row: ee.Feature(ee.Geometry.Point([row[lon_col], row[lat_col]])), axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-english",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in positives.index:\n",
    "    locRow = positives[positives.index == i]\n",
    "    export_loc_data_S2(locRow, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in negatives.index:\n",
    "    locRow = negatives[negatives.index == i]\n",
    "    export_loc_data_S2(locRow, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-planning",
   "metadata": {},
   "source": [
    "## Generating Planet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_location_data(cnx, limit=100):\n",
    "\n",
    "    query_positive = f'''\n",
    "    SELECT *\n",
    "    FROM `mai-database`.`location_file` lf\n",
    "    WHERE EXISTS(\n",
    "        SELECT 1 FROM process_runs pr\n",
    "        WHERE pr.Location = lf.Location\n",
    "        AND Setup='exportAct5' \n",
    "        AND Process=\"04ActivityExport\"\n",
    "        AND Status=\"complete\"\n",
    "\n",
    "    )\n",
    "    LIMIT {limit}\n",
    "    '''\n",
    "    positives = locShpFromQuery(query_positive)\n",
    "    \n",
    "    query_negative = f'''\n",
    "    SELECT *\n",
    "    FROM `mai-database`.`location_file` lf\n",
    "    WHERE EXISTS(\n",
    "        SELECT 1 FROM process_runs pr\n",
    "        WHERE pr.Location = lf.Location\n",
    "        AND Setup='Apr24' \n",
    "        AND Process=\"01Prep\"\n",
    "        AND Status=\"complete\"\n",
    "    )\n",
    "    AND  NOT EXISTS(\n",
    "        SELECT 1 FROM  process_runs pr\n",
    "        WHERE pr.Location = lf.Location\n",
    "        AND Setup='exportAct5' \n",
    "        AND Process=\"04ActivityExport\"\n",
    "    )\n",
    "    LIMIT {limit}\n",
    "    '''\n",
    "    negatives = locShpFromQuery(query_negative)\n",
    "    \n",
    "    return positives, negatives\n",
    "\n",
    "positives, negatives = fetch_location_data(cnx, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Image('projects/p155mali11/assets/PS_imgs/155_Mali/lon-0_0037lat16_256proc/diffImgApr24').bandNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_loc_data_planet(locRow):\n",
    "    \n",
    "    loc = locRow.iloc[0]['mktID']\n",
    "    country = locRow.iloc[0]['country']\n",
    "    bucket = locRow.iloc[0]['bucket']\n",
    "    csv_path = './training_data_planet/image_metadata.csv'\n",
    "\n",
    "    original_names = ['b0_50p_max_pMax_wd0','b1_50p_max_pMax_wd1','b2_50p_max_pMax_wd2','b3_50p_max_pMax_wd3',\n",
    "                      'b4_50p_max_pMax_wd4','b5_50p_max_pMax_wd5','b6_50p_max_pMax_wd6']\n",
    "    new_names = ['weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', \n",
    "                 'weekday_4', 'weekday_5', 'weekday_6']\n",
    "    \n",
    "    vis_params = {\n",
    "        'min': 0,\n",
    "        'max': 4,\n",
    "        'palette': ['black', 'white']  # Set a default grayscale palette\n",
    "    }\n",
    "\n",
    "    marketDays = locRow.iloc[0]['marketDays']\n",
    "\n",
    "    #create negative output if result is negative\n",
    "#     try:\n",
    "    if marketDays == 'None':\n",
    "        dayOutput = [0,0,0,0,0,0,0]\n",
    "        mktOutput = 0\n",
    "    else:\n",
    "        dayOutput, marketDayList = transform_to_val_output(marketDays)\n",
    "        mktOutput = 1\n",
    "\n",
    "    # import diffImg\n",
    "    sub_folder = ee.data.listAssets(f'projects/{bucket}/assets/PS_imgs/')['assets'][0]['id']\n",
    "    asset_id = f\"{sub_folder}/{loc}proc/diffImgApr24\"\n",
    "    image = ee.Image(asset_id)\n",
    "    region = image.geometry()\n",
    "    # for each remaining cell in the grid, export diffImg\n",
    "    filename = loc\n",
    "    if check_filename_in_csv(filename, csv_path):\n",
    "        print(f'Image {filename} is already exported.')\n",
    "    else:\n",
    "        locGeo = geemap.geopandas_to_ee(locRow).geometry().bounds()\n",
    "        image = ee.Image(asset_id).clip(locGeo).unmask(ee.Image.constant(0))\n",
    "        folder = 'training_data_planet/images'\n",
    "        export_image_and_data(image, folder, filename, dayOutput, mktOutput, 3.1, csv_path, country)\n",
    "#     except:\n",
    "#         print(f'Failed to export image for {loc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = geemap.Map()\n",
    "m.center_object(locGeo)\n",
    "m.add_layer(locGeo, {'color': 'red'})\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "locRow = negatives[negatives.index == 0]\n",
    "loc = locRow.iloc[0]['mktID']\n",
    "country = locRow.iloc[0]['country']\n",
    "bucket = locRow.iloc[0]['bucket']\n",
    "csv_path = './training_data_planet/image_metadata.csv'\n",
    "\n",
    "original_names = ['b0_50p_max_pMax_wd0','b1_50p_max_pMax_wd1','b2_50p_max_pMax_wd2','b3_50p_max_pMax_wd3',\n",
    "                  'b4_50p_max_pMax_wd4','b5_50p_max_pMax_wd5','b6_50p_max_pMax_wd6']\n",
    "new_names = ['weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', \n",
    "             'weekday_4', 'weekday_5', 'weekday_6']\n",
    "\n",
    "vis_params = {\n",
    "    'min': 0,\n",
    "    'max': 4,\n",
    "    'palette': ['black', 'white']  # Set a default grayscale palette\n",
    "}\n",
    "\n",
    "marketDays = locRow.iloc[0]['marketDays']\n",
    "\n",
    "#create negative output if result is negative\n",
    "#     try:\n",
    "if marketDays == 'None':\n",
    "    dayOutput = [0,0,0,0,0,0,0]\n",
    "    mktOutput = 0\n",
    "else:\n",
    "    dayOutput, marketDayList = transform_to_val_output(marketDays)\n",
    "    mktOutput = 1\n",
    "\n",
    "# import diffImg\n",
    "sub_folder = ee.data.listAssets(f'projects/{bucket}/assets/PS_imgs/')['assets'][0]['id']\n",
    "asset_id = f\"{sub_folder}/{loc}proc/diffImgApr24\"\n",
    "image = ee.Image(asset_id)\n",
    "region = image.geometry()\n",
    "# for each remaining cell in the grid, export diffImg\n",
    "filename = loc\n",
    "if check_filename_in_csv(filename, csv_path):\n",
    "    print(f'Image {filename} is already exported.')\n",
    "else:\n",
    "    locGeo = geemap.geopandas_to_ee(locRow).geometry().centroid().bounds()\n",
    "    image = ee.Image(asset_id).clip(locGeo)\n",
    "    folder = 'training_data_planet/images'\n",
    "    export_image_and_data(image, folder, filename, dayOutput, mktOutput, 3.1, csv_path, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.geopandas_to_ee("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-storage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in list(negatives.index):\n",
    "    locRow = negatives[negatives.index == i]\n",
    "    export_loc_data_planet(locRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(positives.index):\n",
    "    locRow = positives[positives.index == i]\n",
    "    export_loc_data_planet(locRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-house",
   "metadata": {},
   "source": [
    "## Checks and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_tiff_bands(file_path):\n",
    "    \"\"\"\n",
    "    Visualizes each band of a TIFF file arranged in two rows, without a color bar.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the TIFF file.\n",
    "    \"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        num_bands = src.count\n",
    "        \n",
    "        # Prepare to store min and max values across all bands\n",
    "        min_val, max_val = float('inf'), float('-inf')\n",
    "        \n",
    "        # Pre-read all bands to determine global min and max\n",
    "        bands = []\n",
    "        for i in range(1, num_bands + 1):\n",
    "            band = src.read(i)\n",
    "            bands.append(band)\n",
    "            min_val = 0 #min(min_val, band.min())\n",
    "            max_val = 2 #max(max_val, band.max())\n",
    "        \n",
    "        # Determine layout for subplots based on the number of bands\n",
    "        if num_bands <= 4:\n",
    "            rows, cols = 1, num_bands\n",
    "        else:\n",
    "            rows, cols = 2, (num_bands + 1) // 2  # Adjust based on number of bands\n",
    "        \n",
    "        # Create a larger figure with subplots arranged in two rows\n",
    "        fig, axs = plt.subplots(rows, cols, figsize=(20, 10))\n",
    "\n",
    "        # Flatten axs array for easy iteration, in case of just one row axs is not an array\n",
    "        axs = axs.flatten() if num_bands > 1 else [axs]\n",
    "        \n",
    "        # Iterate through each band and visualize with the same color scale\n",
    "        for i, band in enumerate(bands):\n",
    "            axs[i].imshow(band, cmap='gray', vmin=min_val, vmax=max_val)  # Uniform color scaling\n",
    "            axs[i].set_title(f'Band {i + 1}')\n",
    "            axs[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()  # Adjust layout to fit everything nicely\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "file_path = 'training_data_planet/images/lon36_7349lat-1_1427.tif'  # Update this to the path of your TIFF file\n",
    "visualize_tiff_bands(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "def print_band_values(file_path, band_number):\n",
    "    \"\"\"\n",
    "    Prints the values of a specific band from a TIFF file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the TIFF file.\n",
    "        band_number (int): The band number to display.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of values for the specified band.\n",
    "    \"\"\"\n",
    "    # Open the TIFF file\n",
    "    with rasterio.open(file_path) as src:\n",
    "        # Check if the band number is valid\n",
    "        if band_number < 1 or band_number > src.count:\n",
    "            raise ValueError(f\"Invalid band number: {band_number}. This file has {src.count} bands.\")\n",
    "        \n",
    "        # Read the specified band\n",
    "        band_data = src.read(band_number)\n",
    "        \n",
    "        # Optionally, print the data\n",
    "        print(\"Band\", band_number, \"data:\")\n",
    "        print(band_data)\n",
    "        \n",
    "        # Return the array of values\n",
    "        return band_data\n",
    "\n",
    "# Call the function and get the band data as an array\n",
    "band_data = print_band_values('training_data_planet/images/lon36_7349lat-1_1427.tif', 5)\n",
    "print(band_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = './training_data_S2/image_metadata.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    path = row['image_file_path']\n",
    "    loc = '_'.join(path.replace('training_data/images/', '').replace('.tif', '').split('_')[0:-1])\n",
    "\n",
    "    try:\n",
    "        country = positives[positives['mktID'] == loc].iloc[0]['country']\n",
    "    except:\n",
    "        country = negatives[negatives['mktID'] == loc].iloc[0]['country']\n",
    "    \n",
    "    df.at[index, 'country'] = round(min_val, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in df['image_file_path']:\n",
    "    loc = '_'.join(path.replace('training_data/images/', '').replace('.tif', '').split('_')[0:-1])\n",
    "    try:\n",
    "        country = positives[positives['mktID'] == loc].iloc[0]['country']\n",
    "    except:\n",
    "        country = negatives[negatives['mktID'] == loc].iloc[0]['country']\n",
    "    df[df['image_file_path'] == path]['country'] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store min and max values\n",
    "min_values = []\n",
    "max_values = []\n",
    "\n",
    "# Function to get min and max pixel values from a .tif file\n",
    "def get_min_max_std(image_path):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        data = src.read(1)  # Read the first layer\n",
    "        min_val = data.min()\n",
    "        max_val = data.max()\n",
    "        std_dev = np.std(data)\n",
    "    return min_val, max_val, std_dev\n",
    "\n",
    "get_min_max_std('training_data/images/lon-0_1301lat16_5793_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame and update it\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row['image_file_path']\n",
    "    if os.path.exists(image_path):\n",
    "        min_val, max_val, std_dev = get_min_max_std(image_path)\n",
    "        df.at[index, 'min_value'] = round(min_val, 3)\n",
    "        df.at[index, 'max_value'] = round(max_val, 3)\n",
    "        df.at[index, 'std_dev'] = round(std_dev, 3)\n",
    "    else:\n",
    "        # If file does not exist, append NaN or placeholders\n",
    "        df.at[index, 'min_value'] = float('nan')\n",
    "        df.at[index, 'max_value'] = float('nan')\n",
    "        df.at[index, 'std_dev'] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['min_value'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-norman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"CSV file has been updated with min and max values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-corner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory containing the .tif files\n",
    "directory_path = './training_data/images'\n",
    "\n",
    "# Loop through the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.tif'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "\n",
    "print(\"All .tif files have been deleted from the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the .tif files\n",
    "directory_path = './training_data/images'\n",
    "# Path to the CSV file\n",
    "csv_file_path = './training_data/image_metadata.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Loop through the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.tif'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Check if the TIFF file is filled only with zeros\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data = src.read()  # Read all bands\n",
    "            if (data == 0).all():  # Check if all pixel values across all bands are 0\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "                \n",
    "                # Remove the corresponding row from the CSV file\n",
    "                df = df[df['image_file_path'] != file_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(\"Updated CSV file and deleted relevant .tif files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ea723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import tifffile\n",
    "\n",
    "def find_median_pixel(folder_path, sample_size=100):\n",
    "    # Get all .tif files in the folder\n",
    "    tiff_files = glob.glob(os.path.join(folder_path, '*.tif'))\n",
    "    \n",
    "    # If there are more than 'sample_size' files, sample them randomly\n",
    "    if len(tiff_files) > sample_size:\n",
    "        tiff_files = random.sample(tiff_files, sample_size)\n",
    "    \n",
    "    # Collect all pixel values from the selected files\n",
    "    all_pixels = []\n",
    "    for tif_file in tiff_files:\n",
    "        # Read the image as a NumPy array\n",
    "        img = tifffile.imread(tif_file)\n",
    "        # Flatten and add to the list\n",
    "        all_pixels.append(img.flatten())\n",
    "    \n",
    "    # Combine all pixel values into one large array\n",
    "    all_pixels = np.concatenate(all_pixels)\n",
    "    \n",
    "    # Filter out zero values\n",
    "    nonzero_pixels = all_pixels[all_pixels != 0]\n",
    "    \n",
    "    # If there are no nonzero pixels, handle this case\n",
    "    if nonzero_pixels.size == 0:\n",
    "        return None  # or handle as you see fit, e.g. return 0 or raise an error\n",
    "    \n",
    "    # Compute and return the median of nonzero pixel values\n",
    "    median_val = np.percentile(nonzero_pixels, 0.19)\n",
    "    return median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb14420",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_median_pixel('training_data_S2/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-captain",
   "metadata": {},
   "source": [
    "# Planet DiffImgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
