{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee, geemap, pickle, requests\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from io import BytesIO\n",
    "from shapely.geometry import mapping\n",
    "from data_loader import preprocess_new_data, TFDatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/siamese_fusion_anymarket_2/20250211'\n",
    "model_name = model_path.split('/')[-2]\n",
    "model = tf.keras.models.load_model(f'{model_path}/{model_name}.keras')\n",
    "with open(f'{model_path}/scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "with open(f'{model_path}/args.pkl', 'rb') as file:\n",
    "    args = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_predictions_country(candidate_locs_folder, model_path):\n",
    "\n",
    "    # Load model, scaler, and metadata from the model folder\n",
    "    model_name = model_path.split('/')[-2]\n",
    "    model_date = model_path.split('/')[-1]\n",
    "    model = tf.keras.models.load_model(f'{model_path}/{model_name}.keras')\n",
    "    with open('models/siamese_fusion_anymarket_2/20250213/scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    with open('models/siamese_fusion_anymarket_2/20250213/args.pkl', 'rb') as file:\n",
    "        args = pickle.load(file)\n",
    "    predictions_folder = f'{candidate_locs_folder}/S2/cnn_predictions'\n",
    "\n",
    "    # If predictions folder does not exist, create it\n",
    "    if not ee.data.getInfo(predictions_folder):\n",
    "        print('Creating folder:', predictions_folder)\n",
    "        ee.data.createFolder(predictions_folder)\n",
    "\n",
    "    # If model folder does not exist, create it\n",
    "    if not ee.data.getInfo(f'{predictions_folder}/{model_name}'):\n",
    "        print('Creating folder:', f'{predictions_folder}/{model_name}')\n",
    "        ee.data.createFolder(f'{predictions_folder}/{model_name}')\n",
    "\n",
    "    # If model subfolder does not exist, create it\n",
    "    if not ee.data.getInfo(f'{predictions_folder}/{model_name}/{model_date}'):\n",
    "        print('Creating folder:', f'{predictions_folder}/{model_name}/{model_date}')\n",
    "        ee.data.createFolder(f'{predictions_folder}/{model_name}/{model_date}')\n",
    "\n",
    "    # Get list of diffImgs for this country\n",
    "    asset_list = ee.data.listAssets(f'{candidate_locs_folder}/S2/diffImgs/')\n",
    "    diffImg_list = [asset['id'] for asset in asset_list['assets'] if 'norm' not in asset['id']]\n",
    "\n",
    "    # For each diffImg, make predictions and export to GEE\n",
    "    for diffImg_id in diffImg_list:\n",
    "        \n",
    "        diffImg_name = diffImg_id.split('/')[-1]\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "            # If predictions do not exist already, make them\n",
    "            if not ee.data.getInfo(f'{predictions_folder}/{model_name}/{model_date}' + f'/predictions_{diffImg_name}'):\n",
    "                \n",
    "                print('Processing:', diffImg_name)\n",
    "                diffImg = ee.Image(diffImg_id)\n",
    "                predictions_ee = predictions_from_diffImg(diffImg, model, scaler, args)\n",
    "\n",
    "                task = ee.batch.Export.table.toAsset(\n",
    "                    collection=predictions_ee,\n",
    "                    description=f'predictions_{diffImg_name}_{model_name}',\n",
    "                    assetId=f'{predictions_folder}/{model_name}/{model_date}' + f'/predictions_{diffImg_name}',\n",
    "                )\n",
    "                task.start()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error with {diffImg_name}')\n",
    "            print(e)\n",
    "        \n",
    "\n",
    "def predictions_from_diffImg(diffImg, model, scaler, args):\n",
    "\n",
    "    tile_size = 128 \n",
    "    pixel_size = 3\n",
    "    tile_length = pixel_size * tile_size\n",
    "    image_geo = diffImg.geometry()\n",
    "\n",
    "    # Create an offset grid covering the country\n",
    "    grid = image_geo.coveringGrid(proj=ee.Projection('EPSG:4326').atScale(tile_length))\n",
    "    offset_grid = image_geo.buffer(tile_size*pixel_size*0.5) \\\n",
    "        .coveringGrid(proj=ee.Projection('EPSG:4326') \\\n",
    "        .translate(tile_length, tile_length) \\\n",
    "        .atScale(tile_length))\n",
    "    total_grid = grid.merge(offset_grid)\n",
    "\n",
    "    # Create a mask of where the image exceeds 2\n",
    "    high_pixels = diffImg.select('max_all').gt(2).selfMask()\n",
    "\n",
    "    high_pixels_geo = high_pixels.reduceToVectors(\n",
    "        scale = 3,\n",
    "        geometry = image_geo,\n",
    "        maxPixels = 1e12,\n",
    "        reducer = ee.Reducer.countEvery()\n",
    "    ).geometry()\n",
    "\n",
    "    # Filter the grid to only include the high pixels\n",
    "    high_pixels_grid = total_grid.filterBounds(high_pixels_geo)\n",
    "\n",
    "    # Convert high_pixels_grid to GeoJSON\n",
    "    high_pixels_grid_geojson = high_pixels_grid.getInfo()\n",
    "\n",
    "    # Convert GeoJSON to GeoPandas DataFrame\n",
    "    gdf = gpd.GeoDataFrame.from_features(high_pixels_grid_geojson['features'])\n",
    "\n",
    "    predictions_gdf = predict_gdf_batch(gdf, diffImg, model, scaler, args)\n",
    "\n",
    "    predictions_ee = gdf_to_ee_feature_collection(predictions_gdf)\n",
    "    \n",
    "    return predictions_ee\n",
    "\n",
    "\n",
    "def convert_row_to_ee_geometry(gdf, row_number):\n",
    "    # Get the geometry of the specified row\n",
    "    row_geometry = gdf.iloc[row_number].geometry\n",
    "\n",
    "    # Convert the geometry to GeoJSON format\n",
    "    row_geojson = mapping(row_geometry)\n",
    "\n",
    "    # Create an Earth Engine geometry object\n",
    "    row_ee_geometry = ee.Geometry(row_geojson)\n",
    "    \n",
    "    return row_ee_geometry\n",
    "\n",
    "\n",
    "def get_image_data(image, geometry, scale=3):\n",
    "    # Clip the image to the specified geometry\n",
    "    clipped_image = image.clip(geometry)\n",
    "    \n",
    "    # Get the image data as a numpy array\n",
    "    url = clipped_image.getDownloadURL({\n",
    "        'scale': scale, \n",
    "        'format': 'NPY',\n",
    "        'bands': [\n",
    "            {'id':'weekday_0', 'scale':3},\n",
    "            {'id':'weekday_1', 'scale':3},\n",
    "            {'id':'weekday_2', 'scale':3},\n",
    "            {'id':'weekday_3', 'scale':3},\n",
    "            {'id':'weekday_4', 'scale':3},\n",
    "            {'id':'weekday_5', 'scale':3},\n",
    "            {'id':'weekday_6', 'scale':3}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = np.load(BytesIO(response.content))\n",
    "    \n",
    "    # Reshape the data to have the shape (7, 129, 129)\n",
    "    new_data = data[['weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6']]\n",
    "    new_data = new_data.view((float, len(new_data.dtype.names))).reshape(7, data.shape[0], data.shape[1])\n",
    "    \n",
    "    # Check if the data shape is at least (_, 128, 128)\n",
    "    if new_data.shape[1] < 128 or new_data.shape[2] < 128:\n",
    "        # Calculate the padding sizes\n",
    "        pad_width = ((0, 0), (0, max(0, 128 - new_data.shape[1])), (0, max(0, 128 - new_data.shape[2])))\n",
    "        # Pad the data with 0 values\n",
    "        new_data = np.pad(new_data, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def predict_image_data(data, model, scaler, args):\n",
    "\n",
    "    if data.shape[1] < 128 or data.shape[2] < 128:\n",
    "        raise ValueError('Data must have a shape of at least (7, 128, 128)')\n",
    "\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_new_data(data, scaler=scaler, do_clipping=args.do_clipping, lower_clip=args.lower_clip, upper_clip=args.upper_clip)\n",
    "    \n",
    "    # Predict the data\n",
    "    prediction = model.predict(np.expand_dims(processed_data, axis=0))\n",
    "    \n",
    "    return prediction[0][0]\n",
    "\n",
    "\n",
    "def predict_gdf_batch(gdf, image, model, scaler, args):\n",
    "    processed_data_list = []\n",
    "\n",
    "    for i in range(len(gdf)):\n",
    "        geometry = convert_row_to_ee_geometry(gdf, i)\n",
    "        image_data = get_image_data(image, geometry)\n",
    "        processed_data = preprocess_new_data(image_data, scaler=scaler, do_clipping=args.do_clipping, lower_clip=args.lower_clip, upper_clip=args.upper_clip)\n",
    "        processed_data_list.append(processed_data)\n",
    "\n",
    "    if processed_data_list:\n",
    "        # Stack all image data into a single numpy array\n",
    "        batch_data = np.stack(processed_data_list)\n",
    "\n",
    "        # Predict the data\n",
    "        predictions = model.predict(batch_data)\n",
    "        \n",
    "        # Add predictions to the GeoDataFrame\n",
    "        gdf['prediction'] = predictions\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def gdf_to_ee_feature_collection(gdf):\n",
    "    features = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        geom = ee.Geometry(mapping(row['geometry']))\n",
    "        feature = ee.Feature(geom, row.drop('geometry').to_dict())\n",
    "        features.append(feature)\n",
    "    return ee.FeatureCollection(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = 'Mali'\n",
    "candidate_locs_folder = f'projects/{country_name.lower()}-candidate-locs/assets'\n",
    "\n",
    "cnn_predictions_country(candidate_locs_folder, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 128 \n",
    "pixel_size = 3\n",
    "tile_length = pixel_size * tile_size\n",
    "diffImg = ee.Image('projects/mali-candidate-locs/assets/S2/diffImgs/cell_159')\n",
    "image_geo = diffImg.geometry()\n",
    "\n",
    "# Create an offset grid covering the country\n",
    "grid = image_geo.coveringGrid(proj=ee.Projection('EPSG:4326').atScale(tile_length))\n",
    "offset_grid = image_geo.buffer(tile_size*pixel_size*0.5) \\\n",
    "    .coveringGrid(proj=ee.Projection('EPSG:4326') \\\n",
    "    .translate(tile_length, tile_length) \\\n",
    "    .atScale(tile_length))\n",
    "total_grid = grid.merge(offset_grid)\n",
    "\n",
    "# Create a mask of where the image exceeds 2\n",
    "high_pixels = diffImg.select('max_all').gt(2).selfMask()\n",
    "\n",
    "high_pixels_geo = high_pixels.reduceToVectors(\n",
    "    scale = 3,\n",
    "    geometry = image_geo,\n",
    "    maxPixels = 1e12,\n",
    "    reducer = ee.Reducer.countEvery()\n",
    ").geometry()\n",
    "\n",
    "# Filter the grid to only include the high pixels\n",
    "high_pixels_grid = total_grid.filterBounds(high_pixels_geo)\n",
    "\n",
    "# Map the grid\n",
    "m = geemap.Map()\n",
    "m.addLayer(diffImg, {}, 'diffImg')\n",
    "m.addLayer(high_pixels_grid, {}, 'high_pixels_grid')\n",
    "m.addLayer(high_pixels, {}, 'high_pixels')\n",
    "m.addLayer(high_pixels_geo, {}, 'high_pixels_geo')\n",
    "m.centerObject(diffImg)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pixels_grid.size().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert high_pixels_grid to GeoJSON\n",
    "high_pixels_grid_geojson = high_pixels_grid.getInfo()\n",
    "\n",
    "# Convert GeoJSON to GeoPandas DataFrame\n",
    "gdf = gpd.GeoDataFrame.from_features(high_pixels_grid_geojson['features'])\n",
    "\n",
    "# Print the GeoPandas DataFrame\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "first_row_ee_geometry = convert_row_to_ee_geometry(gdf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "data = get_image_data(diffImg, first_row_ee_geometry)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model('models/siamese_fusion_anymarket_2/20250211/siamese_fusion_anymarket_2.keras')\n",
    "model2 = tf.keras.models.load_model('models/siamese_fusion_anymarket_2/20250213/siamese_fusion_anymarket_2.keras')\n",
    "\n",
    "# Load the model\n",
    "model = model2\n",
    "\n",
    "# Min max scaler, min = 0, max = 40\n",
    "class CustomMinMaxScaler:\n",
    "    def __init__(self, min_val=0, max_val=40):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.min_val) / (self.max_val - self.min_val)\n",
    "\n",
    "scaler = CustomMinMaxScaler()\n",
    "\n",
    "def predict_image_data(data, model, scaler):\n",
    "\n",
    "    if data.shape[1] < 128 or data.shape[2] < 128:\n",
    "        raise ValueError('Data must have a shape of at least (7, 128, 128)')\n",
    "\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_new_data(data, scaler=scaler, do_clipping=True, lower_clip=0, upper_clip=40)\n",
    "\n",
    "    # Plot the value distribution of processed_data\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.hist(processed_data.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "    # plt.title('Value Distribution of Processed Data')\n",
    "    # plt.xlabel('Value')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    # Predict the data\n",
    "    prediction = model.predict(np.expand_dims(processed_data, axis=0))\n",
    "    \n",
    "    return prediction[0][0]\n",
    "\n",
    "geometry = convert_row_to_ee_geometry(gdf, 0)\n",
    "image_data = get_image_data(diffImg, geometry)\n",
    "predict_image_data(image_data, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gdf_batch(gdf, image, model, scaler):\n",
    "    processed_data_list = []\n",
    "\n",
    "    for i in range(len(gdf)):\n",
    "        geometry = convert_row_to_ee_geometry(gdf, i)\n",
    "        image_data = get_image_data(image, geometry)\n",
    "        processed_data = preprocess_new_data(image_data, scaler=scaler, do_clipping=True, lower_clip=0, upper_clip=40)\n",
    "        processed_data_list.append(processed_data)\n",
    "\n",
    "    if processed_data_list:\n",
    "        # Stack all image data into a single numpy array\n",
    "        batch_data = np.stack(processed_data_list)\n",
    "\n",
    "        # Predict the data\n",
    "        predictions = model.predict(batch_data)\n",
    "        \n",
    "        # Add predictions to the GeoDataFrame\n",
    "        gdf['predictions'] = predictions\n",
    "\n",
    "    return gdf\n",
    "\n",
    "predictions = predict_gdf_batch(gdf, diffImg, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdf_to_ee_feature_collection(gdf):\n",
    "    features = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        geom = ee.Geometry(mapping(row['geometry']))\n",
    "        feature = ee.Feature(geom, row.drop('geometry').to_dict())\n",
    "        features.append(feature)\n",
    "    return ee.FeatureCollection(features)\n",
    "\n",
    "# Convert the GeoPandas DataFrame to an Earth Engine Feature Collection\n",
    "predictions_ee = gdf_to_ee_feature_collection(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_locs_existing = ee.FeatureCollection(f'projects/{country_name.lower()}-candidate-locs/assets/S2/locs/cell_159_locs_v20240812')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = geemap.Map()\n",
    "empty = ee.Image().byte()\n",
    "outline = empty.paint(featureCollection=predictions_ee, color='predictions')\n",
    "m.addLayer(diffImg.select('max_all'), {'max': 10}, 'diffImg')\n",
    "m.addLayer(predictions_ee.filter(ee.Filter.gt('predictions', 0.2)), {'color': 'cyan'},'high_predictions')\n",
    "m.addLayer(outline, {'palette': ['black', 'orange']}, 'predictions', False)\n",
    "m.addLayer(candidate_locs_existing, {'color': 'red'}, 'candidate_locs_existing')\n",
    "m.centerObject(diffImg)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the centroid of any prediction > 0.2\n",
    "centroids = predictions_ee.filter(ee.Filter.gt('predictions', 0.2)).map(lambda feature: feature.centroid())\n",
    "candidate_locs_initial = centroids.map(lambda feature: feature.buffer(250))\n",
    "\n",
    "def determine_overlapping(feature):\n",
    "    num_ovelapping = candidate_locs_initial.filterBounds(feature.geometry()).size()\n",
    "    return feature.set('num_overlapping', num_ovelapping)\n",
    "\n",
    "candidate_locs_overlapping = candidate_locs_initial.map(determine_overlapping).filter(ee.Filter.gt('num_overlapping', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = geemap.Map()\n",
    "m.addLayer(diffImg.select('max_all'), {'max': 10}, 'diffImg')\n",
    "m.addLayer(outline, {'palette': ['black', 'orange']}, 'predictions')\n",
    "m.addLayer(candidate_locs, {}, 'candidate_locs')\n",
    "m.centerObject(diffImg)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data for each weekday\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "band_names = diffImg.bandNames().getInfo()\n",
    "\n",
    "# Visualize each weekday layer\n",
    "for band in band_names[:-1]:  # Exclude 'max_all'\n",
    "    plt.figure()\n",
    "    plt.imshow(data[band], cmap='viridis')\n",
    "    plt.title(f'{band} layer')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure new_data is defined\n",
    "new_data = data[['weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6']]\n",
    "\n",
    "new_data = new_data.view((float, len(new_data.dtype.names))).reshape(7, 129, 129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to make sure the prediction is correct\n",
    "m = geemap.Map()\n",
    "m.addLayer(diffImg.select('max_all'), {'max':15}, 'diffImg')\n",
    "m.addLayer(grid_cell, {}, 'grid_cell')\n",
    "m.centerObject(grid_cell)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import TFDatasetBuilder\n",
    "\n",
    "\n",
    "builder = TFDatasetBuilder(\n",
    "        csv_path=\"./data/training_data_S2/image_metadata.csv\",\n",
    "        scaling='minmax',\n",
    "        do_augmentation = True,\n",
    "        do_clipping = True,\n",
    "        lower_clip = 0,\n",
    "        upper_clip = 40\n",
    "    )\n",
    "\n",
    "train_ds, val_ds, test_ds = builder.build_datasets(\n",
    "        train_split=0.70, val_split=0.15,\n",
    "        sample_size=100,\n",
    "        batch_size=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_image_from_dataset(dataset, index=0):\n",
    "    # Get a batch of images and labels from the dataset\n",
    "    for images, labels in dataset.take(1):\n",
    "        image = images[index].numpy()\n",
    "        true_label = labels[index].numpy()\n",
    "        break\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(image.shape[0]):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(image[i, :, :, 0], cmap='viridis')\n",
    "        plt.title(f'Layer {i}')\n",
    "        plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the histogram of the image values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(image.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "    plt.title('Value Distribution of Image')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the true label\n",
    "    print(f'True Label: {true_label}')\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(np.expand_dims(image, axis=0))\n",
    "    print(f'Prediction: {prediction[0][0]}')\n",
    "\n",
    "# Example usage\n",
    "visualize_image_from_dataset(train_ds, index=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model('models/siamese_fusion_anymarket_2/20250211/siamese_fusion_anymarket_2.keras')\n",
    "model2 = tf.keras.models.load_model('models/siamese_fusion_anymarket_2/20250213/siamese_fusion_anymarket_2.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
